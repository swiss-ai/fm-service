[["Map",1,2,9,10,69,70],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.7.10","content-config-digest","b747f7bb2f0d09ff","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://research.computer\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\",\"entrypoint\":\"astro/assets/endpoint/node\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false},\"session\":{\"driver\":\"fs-lite\",\"options\":{\"base\":\"/home/bunt/projects/fm-service/apps/home/node_modules/.astro/sessions\"}}}","articles",["Map",11,12,45,46],"02-hexgen",{"id":11,"data":13,"body":15,"filePath":29,"digest":30,"rendered":31,"legacyId":44},{"title":14,"description":15,"date":16,"demoURL":17,"repoURL":18,"authors":19},"HexGen: Generative Inference of Large Language Model over Heterogeneous Environment","Serving generative inference of the large language model is a crucial component of contemporary AI applications. This paper focuses on deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism and allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive evaluation to verify the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The results suggest that HexGen can choose to achieve up to 2.3 times lower latency deadlines or tolerate up to 4 times more request rates compared with the homogeneous baseline given the same budget.",["Date","2024-03-21T23:00:00.000Z"],"https://arxiv.org/abs/2311.11514","https://github.com/Relaxed-System-Lab/HexGen",[20,23,26],{"name":21,"url":22},"Youhe Jiang","https://youhe-jiang.github.io/",{"name":24,"url":25},"Yan Ran","https://ranyangit.github.io/",{"name":27,"url":28},"Xiaozhe Yao","https://about.yao.sh/","src/content/articles/02-hexgen/index.md","a9ca4391e46b0b1f",{"html":32,"metadata":33},"\u003Cp>Serving generative inference of the large language model is a crucial component of contemporary AI applications. This paper focuses on deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism and allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive evaluation to verify the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The results suggest that HexGen can choose to achieve up to 2.3 times lower latency deadlines or tolerate up to 4 times more request rates compared with the homogeneous baseline given the same budget.\u003C/p>",{"headings":34,"localImagePaths":35,"remoteImagePaths":36,"frontmatter":37,"imagePaths":43},[],[],[],{"title":14,"description":15,"date":38,"authors":39,"demoURL":17,"repoURL":18},"Mar 22 2024",[40,41,42],{"name":21,"url":22},{"name":24,"url":25},{"name":27,"url":28},[],"02-hexgen/index.md","01-deltazip",{"id":45,"data":47,"body":55,"filePath":56,"digest":57,"rendered":58,"legacyId":68},{"title":48,"description":49,"date":50,"demoURL":51,"repoURL":52,"authors":53},"DeltaZip: Multi Full Fine-tuned LLM Serving","Fine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.",["Date","2024-03-21T23:00:00.000Z"],"https://arxiv.org/abs/2312.05215","https://github.com/eth-easl/deltazip",[54],{"name":27,"url":28},"[Model Zoo](https://huggingface.co/deltazip).\n\nFine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.","src/content/articles/01-deltazip/index.md","dd1b4a4285836d6a",{"html":59,"metadata":60},"\u003Cp>\u003Ca href=\"https://huggingface.co/deltazip\">Model Zoo\u003C/a>.\u003C/p>\n\u003Cp>Fine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.\u003C/p>",{"headings":61,"localImagePaths":62,"remoteImagePaths":63,"frontmatter":64,"imagePaths":67},[],[],[],{"title":48,"description":49,"date":38,"authors":65,"demoURL":51,"repoURL":52},[66],{"name":27,"url":28},[],"01-deltazip/index.md","guides",["Map",71,72,96,97],"01-getting-started",{"id":71,"data":73,"body":76,"filePath":77,"digest":78,"rendered":79,"legacyId":95},{"title":74,"description":74,"date":75},"Getting Started",["Date","2024-03-17T23:00:00.000Z"],"# Getting Started\n\n## Run your Model Locally\n\nWith post-ampere GPU (using [Scratchpad](https://github.com/eth-easl/Scratchpad) as backend):\n\n```bash\ndocker run --rm --gpus all --runtime=nvidia -v $OCF_MODELS:/models -e HF_MODELS=/models -e HF_TOKEN=$HF_TOKEN ghcr.io/researchcomputer/ocf-scratchpad:latest \"sp serve\nQwen/Qwen2.5-7B-Instruct-1M --port 8080 --max-prefill-tokens 8192 --context-length 8192\"\n```\n\nWith pre-ampere GPU (using [Ollama](https://ollama.com/) as backend):\n\n```bash\ndocker run --rm --gpus all --runtime=nvidia -v $OCF_MODELS:/models -e OLLAMA_MODELS=/models ghcr.io/researchcomputer/ocf-ollama:latest gemma3:1b\n```","src/content/guides/01-getting-started/index.md","feac32b0025c069a",{"html":80,"metadata":81},"\u003Ch1 id=\"getting-started\">Getting Started\u003C/h1>\n\u003Ch2 id=\"run-your-model-locally\">Run your Model Locally\u003C/h2>\n\u003Cp>With post-ampere GPU (using \u003Ca href=\"https://github.com/eth-easl/Scratchpad\">Scratchpad\u003C/a> as backend):\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">docker\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> run\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --rm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --gpus\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> all\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --runtime=nvidia\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -v\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $OCF_MODELS\u003C/span>\u003Cspan style=\"color:#9ECBFF\">:/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_MODELS=/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_TOKEN=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$HF_TOKEN \u003C/span>\u003Cspan style=\"color:#9ECBFF\">ghcr.io/researchcomputer/ocf-scratchpad:latest\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"sp serve\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">Qwen/Qwen2.5-7B-Instruct-1M --port 8080 --max-prefill-tokens 8192 --context-length 8192\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>With pre-ampere GPU (using \u003Ca href=\"https://ollama.com/\">Ollama\u003C/a> as backend):\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">docker\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> run\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --rm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --gpus\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> all\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --runtime=nvidia\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -v\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $OCF_MODELS\u003C/span>\u003Cspan style=\"color:#9ECBFF\">:/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> OLLAMA_MODELS=/models\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ghcr.io/researchcomputer/ocf-ollama:latest\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> gemma3:1b\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":82,"localImagePaths":90,"remoteImagePaths":91,"frontmatter":92,"imagePaths":94},[83,86],{"depth":84,"slug":85,"text":74},1,"getting-started",{"depth":87,"slug":88,"text":89},2,"run-your-model-locally","Run your Model Locally",[],[],{"title":74,"description":74,"date":93},"Mar 18 2024",[],"01-getting-started/index.md","01-getting-started/alps",{"id":96,"data":98,"body":101,"filePath":102,"digest":103,"rendered":104,"legacyId":144},{"title":99,"description":74,"date":100},"Spin Up Models on Alps",["Date","2025-05-18T22:00:00.000Z"],"## Quick Start\n\n1. Download the script:\n   ```bash\n   wget https://raw.githubusercontent.com/swiss-ai/model-spinning/refs/heads/main/spin-model.py -O spin-model && chmod 755 spin-model && mv spin-model ~/.local/bin/\n   ```\n\n2. Check your available SLURM accounts:\n   ```bash\n   sacctmgr show associations user=$USER format=user,account%20\n   ```\n\n3. Launch a model:\n\n\n   ```bash\n   # Launch Mistral 7B with tensor parallelism 2 for 30 minutes\n   spin-model --model mistralai/Mistral-7B-Instruct-v0.3 --tp-size 2 --time 30m --account YOUR_ACCOUNT\n   ```\n \n## Usage\n\n```\nusage: spin-model [-h] [--model MODEL] [--time TIME] [--vllm] [--vllm-help]\n                     [--sp-help] [--account ACCOUNT] [--env ENV]\n                     [--environment ENVIRONMENT]\n\nLaunch a model on SLURM\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model MODEL         Name of the model to launch\n  --time TIME           Time duration for the job. Examples: 2h, 1h30m, 90m,\n                        1:30:00\n  --vllm                Use vllm instead of sp to serve the model\n  --vllm-help           Show available options for **vllm** model server\n  --sp-help             Show available options for **sp** model server\n  --account ACCOUNT     Slurm account to use for job submission\n  --env ENV             Specify environment variables in format KEY=VALUE\n  --environment ENVIRONMENT\n                        Specify a custom environment file path\n```\n\nAdditional model-specific arguments can be passed after the main arguments.\n\n## Important Parameters\n\n### Model Serving Options\n\n- **Model Server**: By default, the script uses the `sp` model server. For certain architectures, you can use `--vllm` to switch to the vLLM server.\n- **Documentation**: \n  - [Scratchpad (sp) documentation](https://github.com/swiss-ai/model-spinning/blob/main/sp-docs.txt)\n  - [vLLM documentation](https://github.com/swiss-ai/model-spinning/blob/main/vllm-docs.txt)\n  - View these docs directly with:\n    ```bash\n    spin-model --sp-help    # For sp server options\n    spin-model --vllm-help  # For vllm server options\n    ```\n\n### Tensor Parallelism\n\nThe `--tp-size` parameter specifies the tensor parallelism size when a model is too large to fit on a single GPU:\n\n- Models \u003C 2B parameters: `--tp-size 1`\n- Models \u003C 14B parameters: `--tp-size 2`\n- Models \u003C 45B parameters: `--tp-size 3`\n- Models \u003C 90B parameters: `--tp-size 4`\n\n### Time Allocation\n\nThe `--time` parameter accepts various formats:\n- `2h` (2 hours)\n- `1h30m` (1 hour and 30 minutes)\n- `90m` (90 minutes)\n- `1:30:00` (1 hour and 30 minutes in SLURM format)\n\nNote: On Bristen nodes, time is limited to 1 hour maximum, while Clariden nodes allow up to 24 hours.\n\n### Environment Variables\n\nThe `--env` parameter allows you to specify custom environment variables for your model server. This is useful for:\n\n- Setting API keys (e.g., Hugging Face tokens)\n- Configuring model-specific parameters\n- Passing authentication credentials\n\nYou can specify multiple environment variables by using `--env` multiple times.\n```bash \nspin-model --model CohereLabs/aya-expanse-8b --tensor-parallel-size 2 --time 4h --account YOUR_ACCOUNT --vllm --env HF_TOKEN=hf_abcdef0123456789 --env OPENAI_API_KEY=sk-proj-rniovncziroeuHNOIniuonOIU --env GOOGLE_API_KEY=aoimrewopv_einworcxz\n```\n\n### Model launch examples \n\n```bash\n# Apertus 70B - SwissAI model\nspin-model --model /a10/swiapertus3ss-alignment/checkpoints/apertus3-70B-iter_90000-tulu3-sft/checkpoint-14000 \\\n    --served-model-name swissai/apertus3-70b-0425 \\\n    --account YOUR_ACCOUNT \\\n    --tp-size 4\n\n# Standard model launches (using sp server)\n# Gemma 3 12B - Latest Google model with strong performance\nspin-model --model google/gemma-3-12b-it --tp-size 2 --time 4h --account YOUR_ACCOUNT\n\n# Qwen 2.5 7B \nspin-model --model Qwen/Qwen2.5-7B-Instruct --tp-size 2 --time 4h --account YOUR_ACCOUNT\n\n# DeepSeek 14B - Distilled version of Qwen for better efficiency\nspin-model --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --tp-size 2 --time 4h --account YOUR_ACCOUNT\n\n# vLLM-only architectures (must use --vllm flag)\n# Mistral 7B \nspin-model --model mistralai/Mistral-7B-Instruct-v0.3 --tensor-parallel-size 2 --time 4h --account YOUR_ACCOUNT --vllm\n\n# Phi-3 Mini \nspin-model --model microsoft/Phi-3-mini-4k-instruct --tensor-parallel-size 2 --time 4h --account YOUR_ACCOUNT --vllm\n\n# Gemma 2 9B - Previous generation Google model\nspin-model --model google/gemma-2-9b-it --tensor-parallel-size 2 --time 4h --account YOUR_ACCOUNT --vllm\n\n# Launch Aya Expanse 8B model with vLLM server with a custom variable\nspin-model --model CohereLabs/aya-expanse-8b --tensor-parallel-size 2 --time 4h --account YOUR_ACCOUNT --vllm --env HF_TOKEN=hf_abcdef0123456789\n```\n\n### Local Model Launch and Apertus\n\n```bash\nspin-model --model /a10/swiapertus3ss-alignment/checkpoints/apertus3-70B-iter_90000-tulu3-sft/checkpoint-14000 \\\n    --served-model-name swissai/apertus3-70b-0425 \\\n    --account YOUR_ACCOUNT \\\n    --tp-size 4\n```\n\nThe `--model` parameter specifies the actual path to your model checkpoint. Please, make sure the environment (.toml file) has a mount point at `/a10`. \n\nThe `--served-model-name` parameter allows you to specify a user-friendly name for your model when it's served. \n\n## After Submission\n\nOnce your job is submitted, you'll see:\n- Job ID\n- Commands to check job status and logs","src/content/guides/01-getting-started/alps.md","ec5cdb68582c5347",{"html":105,"metadata":106},"\u003Ch2 id=\"quick-start\">Quick Start\u003C/h2>\n\u003Col>\n\u003Cli>\n\u003Cp>Download the script:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">wget\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> https://raw.githubusercontent.com/swiss-ai/model-spinning/refs/heads/main/spin-model.py\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -O\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> spin-model\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> &#x26;&#x26; \u003C/span>\u003Cspan style=\"color:#B392F0\">chmod\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 755\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> spin-model\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> &#x26;&#x26; \u003C/span>\u003Cspan style=\"color:#B392F0\">mv\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> spin-model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ~/.local/bin/\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003C/li>\n\u003Cli>\n\u003Cp>Check your available SLURM accounts:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">sacctmgr\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> show\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> associations\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> user=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$USER \u003C/span>\u003Cspan style=\"color:#9ECBFF\">format=user,account%20\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003C/li>\n\u003Cli>\n\u003Cp>Launch a model:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Launch Mistral 7B with tensor parallelism 2 for 30 minutes\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> mistralai/Mistral-7B-Instruct-v0.3\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 30m\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"usage\">Usage\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>usage: spin-model [-h] [--model MODEL] [--time TIME] [--vllm] [--vllm-help]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                     [--sp-help] [--account ACCOUNT] [--env ENV]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                     [--environment ENVIRONMENT]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Launch a model on SLURM\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>optional arguments:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  -h, --help            show this help message and exit\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --model MODEL         Name of the model to launch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --time TIME           Time duration for the job. Examples: 2h, 1h30m, 90m,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                        1:30:00\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --vllm                Use vllm instead of sp to serve the model\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --vllm-help           Show available options for **vllm** model server\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --sp-help             Show available options for **sp** model server\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --account ACCOUNT     Slurm account to use for job submission\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --env ENV             Specify environment variables in format KEY=VALUE\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --environment ENVIRONMENT\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                        Specify a custom environment file path\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Additional model-specific arguments can be passed after the main arguments.\u003C/p>\n\u003Ch2 id=\"important-parameters\">Important Parameters\u003C/h2>\n\u003Ch3 id=\"model-serving-options\">Model Serving Options\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Model Server\u003C/strong>: By default, the script uses the \u003Ccode>sp\u003C/code> model server. For certain architectures, you can use \u003Ccode>--vllm\u003C/code> to switch to the vLLM server.\u003C/li>\n\u003Cli>\u003Cstrong>Documentation\u003C/strong>:\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://github.com/swiss-ai/model-spinning/blob/main/sp-docs.txt\">Scratchpad (sp) documentation\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://github.com/swiss-ai/model-spinning/blob/main/vllm-docs.txt\">vLLM documentation\u003C/a>\u003C/li>\n\u003Cli>View these docs directly with:\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --sp-help\u003C/span>\u003Cspan style=\"color:#6A737D\">    # For sp server options\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm-help\u003C/span>\u003Cspan style=\"color:#6A737D\">  # For vllm server options\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"tensor-parallelism\">Tensor Parallelism\u003C/h3>\n\u003Cp>The \u003Ccode>--tp-size\u003C/code> parameter specifies the tensor parallelism size when a model is too large to fit on a single GPU:\u003C/p>\n\u003Cul>\n\u003Cli>Models &#x3C; 2B parameters: \u003Ccode>--tp-size 1\u003C/code>\u003C/li>\n\u003Cli>Models &#x3C; 14B parameters: \u003Ccode>--tp-size 2\u003C/code>\u003C/li>\n\u003Cli>Models &#x3C; 45B parameters: \u003Ccode>--tp-size 3\u003C/code>\u003C/li>\n\u003Cli>Models &#x3C; 90B parameters: \u003Ccode>--tp-size 4\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"time-allocation\">Time Allocation\u003C/h3>\n\u003Cp>The \u003Ccode>--time\u003C/code> parameter accepts various formats:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>2h\u003C/code> (2 hours)\u003C/li>\n\u003Cli>\u003Ccode>1h30m\u003C/code> (1 hour and 30 minutes)\u003C/li>\n\u003Cli>\u003Ccode>90m\u003C/code> (90 minutes)\u003C/li>\n\u003Cli>\u003Ccode>1:30:00\u003C/code> (1 hour and 30 minutes in SLURM format)\u003C/li>\n\u003C/ul>\n\u003Cp>Note: On Bristen nodes, time is limited to 1 hour maximum, while Clariden nodes allow up to 24 hours.\u003C/p>\n\u003Ch3 id=\"environment-variables\">Environment Variables\u003C/h3>\n\u003Cp>The \u003Ccode>--env\u003C/code> parameter allows you to specify custom environment variables for your model server. This is useful for:\u003C/p>\n\u003Cul>\n\u003Cli>Setting API keys (e.g., Hugging Face tokens)\u003C/li>\n\u003Cli>Configuring model-specific parameters\u003C/li>\n\u003Cli>Passing authentication credentials\u003C/li>\n\u003C/ul>\n\u003Cp>You can specify multiple environment variables by using \u003Ccode>--env\u003C/code> multiple times.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> CohereLabs/aya-expanse-8b\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tensor-parallel-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --env\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_TOKEN=hf_abcdef0123456789\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --env\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> OPENAI_API_KEY=sk-proj-rniovncziroeuHNOIniuonOIU\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --env\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> GOOGLE_API_KEY=aoimrewopv_einworcxz\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"model-launch-examples\">Model launch examples\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Apertus 70B - SwissAI model\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /a10/swiapertus3ss-alignment/checkpoints/apertus3-70B-iter_90000-tulu3-sft/checkpoint-14000\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --served-model-name\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> swissai/apertus3-70b-0425\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 4\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Standard model launches (using sp server)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Gemma 3 12B - Latest Google model with strong performance\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> google/gemma-3-12b-it\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Qwen 2.5 7B \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Qwen/Qwen2.5-7B-Instruct\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># DeepSeek 14B - Distilled version of Qwen for better efficiency\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># vLLM-only architectures (must use --vllm flag)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Mistral 7B \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> mistralai/Mistral-7B-Instruct-v0.3\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tensor-parallel-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Phi-3 Mini \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> microsoft/Phi-3-mini-4k-instruct\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tensor-parallel-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Gemma 2 9B - Previous generation Google model\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> google/gemma-2-9b-it\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tensor-parallel-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Launch Aya Expanse 8B model with vLLM server with a custom variable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> CohereLabs/aya-expanse-8b\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --tensor-parallel-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --time\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 4h\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --vllm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --env\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_TOKEN=hf_abcdef0123456789\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"local-model-launch-and-apertus\">Local Model Launch and Apertus\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">spin-model\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --model\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /a10/swiapertus3ss-alignment/checkpoints/apertus3-70B-iter_90000-tulu3-sft/checkpoint-14000\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --served-model-name\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> swissai/apertus3-70b-0425\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --account\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> YOUR_ACCOUNT\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    --tp-size\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 4\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The \u003Ccode>--model\u003C/code> parameter specifies the actual path to your model checkpoint. Please, make sure the environment (.toml file) has a mount point at \u003Ccode>/a10\u003C/code>.\u003C/p>\n\u003Cp>The \u003Ccode>--served-model-name\u003C/code> parameter allows you to specify a user-friendly name for your model when it’s served.\u003C/p>\n\u003Ch2 id=\"after-submission\">After Submission\u003C/h2>\n\u003Cp>Once your job is submitted, you’ll see:\u003C/p>\n\u003Cul>\n\u003Cli>Job ID\u003C/li>\n\u003Cli>Commands to check job status and logs\u003C/li>\n\u003C/ul>",{"headings":107,"localImagePaths":139,"remoteImagePaths":140,"frontmatter":141,"imagePaths":143},[108,111,114,117,121,124,127,130,133,136],{"depth":87,"slug":109,"text":110},"quick-start","Quick Start",{"depth":87,"slug":112,"text":113},"usage","Usage",{"depth":87,"slug":115,"text":116},"important-parameters","Important Parameters",{"depth":118,"slug":119,"text":120},3,"model-serving-options","Model Serving Options",{"depth":118,"slug":122,"text":123},"tensor-parallelism","Tensor Parallelism",{"depth":118,"slug":125,"text":126},"time-allocation","Time Allocation",{"depth":118,"slug":128,"text":129},"environment-variables","Environment Variables",{"depth":118,"slug":131,"text":132},"model-launch-examples","Model launch examples",{"depth":118,"slug":134,"text":135},"local-model-launch-and-apertus","Local Model Launch and Apertus",{"depth":87,"slug":137,"text":138},"after-submission","After Submission",[],[],{"title":99,"description":74,"date":142},"May 19 2025",[],"01-getting-started/alps.md"]